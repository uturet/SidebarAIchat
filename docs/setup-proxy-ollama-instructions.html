<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="color-scheme" content="light dark" />
    <meta name="description" content="How to configure a reverse proxy so Sidebar AI Chat can connect to Ollama models securely." />
    <link rel="icon" type="image/svg+xml" href="/chat.svg" />
    <title>Set up Reverse Proxy for Ollama</title>
    <link rel="stylesheet" href="/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/vs2015.min.css">
  </head>
  <body>
    <header class="site-header">
      <h1 class="brand"><a href="/"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="header-icon">
        <path stroke-linecap="round" stroke-linejoin="round" d="M8.625 12a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Zm0 0H8.25m4.125 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Zm0 0H12m4.125 0a.375.375 0 1 1-.75 0 .375.375 0 0 1 .75 0Zm0 0h-.375M21 12c0 4.556-4.03 8.25-9 8.25a9.764 9.764 0 0 1-2.555-.337A5.972 5.972 0 0 1 5.41 20.97a5.969 5.969 0 0 1-.474-.065 4.48 4.48 0 0 0 .978-2.025c.09-.457-.133-.901-.467-1.226C3.93 16.178 3 14.189 3 12c0-4.556 4.03-8.25 9-8.25s9 3.694 9 8.25Z" />
      </svg>
       Sidebar AI Chat</a></h1>
      <nav class="site-nav"><a href="/docs/" aria-current="page">Docs</a></nav>
      <button id="theme-toggle" aria-label="Toggle theme"></button>
    </header>
    <nav class="breadcrumbs" aria-label="Breadcrumb">
      <ol>
        <li><a href="/">Home</a></li>
        <li><a href="/docs/">Docs</a></li>
        <li><span aria-current="page">Set up Reverse Proxy for Ollama</span></li>
      </ol>
    </nav>
    <main>
      <h2>Set up Reverse Proxy for Ollama</h2>
      <p>Follow these steps to expose Ollama through a secure proxy that speaks the headers Sidebar AI Chat expects.</p>
      <h3>1. Configure Ollama behind a reverse proxy</h3>
      <p>Run Ollama on a host you control and place a reverse proxy (Nginx or similar) in front of it. Point the proxy at the port where Ollama listens, require HTTPS for external traffic, and restrict access to trusted networks.</p>
      <p>Ollama enforces a strict CORS policy and rejects unknown origins. Requests from the Chrome extension include a <code>chrome-extension://</code> origin that Ollama blocks, so the proxy must add CORS headers for that ID.</p>
      <p>If you need to install Nginx first, see <a href="setup-nginx.html">Set up Nginx on Windows, Linux, and macOS</a>.</p>
      <p><strong>Create new file </strong>
        <code class="command" id="nginx-default-path">(detecting OS...)</code></p>
      <div class="code-toolbar">
        <button type="button" class="code-toggle" id="code-toggle" aria-pressed="true" data-code="nginx-proxy">Hide code</button>
        <button type="button" class="code-copy" aria-label="Copy code" data-code="nginx-proxy">Copy</button>
      </div>
      <pre data-code="nginx-proxy"><code class="hljs language-python">server {
        listen 8080;
        server_name localhost;

        client_max_body_size 100M;

        location / {
            if ($request_method = 'OPTIONS') {
                add_header 'Access-Control-Allow-Origin' 'chrome-extension://ID';
                add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE';
                add_header 'Access-Control-Allow-Headers' 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization';
                add_header 'Access-Control-Max-Age' 1728000;
                add_header 'Content-Type' 'text/plain; charset=utf-8';
                add_header 'Content-Length' 0;
                return 204;
            }

            # CORS headers for actual requests
            add_header 'Access-Control-Allow-Origin' 'chrome-extension://ID' always;
            add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS, PUT, DELETE' always;
            add_header 'Access-Control-Allow-Headers' 'DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization' always;
            add_header 'Access-Control-Expose-Headers' 'Content-Length,Content-Range' always;

            # Proxy to Ollama
            proxy_pass http://127.0.0.1:11434;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Remove/override Origin header to prevent leakage
            proxy_set_header Origin "";

            # Handle streaming responses
            proxy_buffering off;
            proxy_cache off;

            # Increase timeouts for long-running requests
            proxy_connect_timeout 60s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
        }
    }
    </code></pre>
      <p>Reload Nginx, open the sidebar, and start a test chat. You should see a streaming response from the model. If the request fails, inspect the browser console and Nginx logs for CORS or proxy errors.</p>
    </main>
    <footer>
      Â© <span id="year"></span> SidebarAIchat.com | <a href="/docs/">Docs</a>
    </footer>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <script src="/script.js"></script>
  </body>
</html>
